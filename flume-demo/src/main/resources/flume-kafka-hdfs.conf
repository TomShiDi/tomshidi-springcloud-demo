# agent
a1.sources=r1
a1.sinks=k1
a1.channels=c1

# source
a1.sources.r1.type=org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.channels=c1
a1.sources.r1.batSize=5000
a1.sources.r1.batchDurationMillis=2000
a1.sources.r1.kafka.bootstrap.servers=node1:9092,node2:9092
a1.sources.r1.kafka.topics=topic_log
a1.sources.r1.kafka.consumer.group.id=flume
a1.sources.r1.interceptors=i1
a1.sources.r1.interceptors.i1.type=com.tomshidi.flume.interceptor.TimeStampInterceptor$Builder

# channel
a1.channels.c1.type=file
a1.channels.c1.checkpointDir=/opt/server/flume/checkpoint/behavior1
a1.channels.c1.dataDirs=/opt/server/flume/data/behavior1
a1.channels.c1.maxFileSize=2146435071
a1.channels.c1.capacity=1000000
a1.channels.c1.keep-alive=6

# sink
a1.sinks.k1.type=hdfs
a1.sinks.k1.hdfs.path=/origin_data/gmall/log/topic_log/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix=events-
a1.sinks.k1.hdfs.round=false
a1.sinks.k1.hdfs.rollInterval=10
a1.sinks.k1.hdfs.rollSize=134217728
a1.sinks.k1.hdfs.rollCount=0

a1.sinks.k1.hdfs.fileType=CompressedStream
a1.sinks.k1.hdfs.codeC=gzip



# bind
a1.sources.r1.channels=c1
a1.sinks.k1.channel=c1
